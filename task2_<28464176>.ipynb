{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name: HsinHui Lin\n",
    "#### Student ID: 28464176\n",
    "#### Date: 02/Sep/2018\n",
    "#### Version: 1.0\n",
    "#### Environment: Python 3.6 and Jupyter notebook\n",
    "#### Libraries used: \n",
    "#### nltk (for symbolic and statistical natural language processing (NLP) , included in Anaconda Python 3.6)\n",
    "#### os (for using operating system dependent functionality, included in Anaconda Python 3.6)\n",
    "#### chain (for receiving an iterable object as a parameter and return an iterator, included in Anaconda Python 3.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ___________________\n",
    "### Import library\n",
    "\n",
    "#### The chain library in the itertool is to receive multiple iterable objects as parameters, \"join\" them, and return as a new iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import *\n",
    "import os\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### name a variable \"todo\" and assign value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "todo=\"850 374 859 661  19 838 190 436 209 800 437 361 294 791 315 168 109 169 681  95 150 718 236 216 651  24 537 485 606 363 642  31 211 160 540 523 484 589 685 456 362 816  47 649 616 567 223 365 481 142 490  15 785 200 115 511 305 617 788 564 133 861 463 704 737 839  23 777 797 496 168 273 796 238 694  74 479 769 154 142  33 763  72 128 543  24 690 672 140 571 640 630  56 482 517 676 338 817 691 696 141  42 839 281 544 293 535 231 186 613 611 134 852 256 238 591  48 476 274 354 419 350  90  38 821 762 397 174 457 764 349 383  80 432 570 384 401  63 635  74 616 770 772 352 186 230 608 661 138 111 830  58 646  63 209 519 772 480 172 658 555 507 587 302 308 544 491 167 819 540 346 517 595 392 563  37 610 513 604 273 584 390  35 863 168 360 189 506 782  41 727 832 768 439 181 427 461  68 210 683 760 840  45 274 432 825 333 363 438 220 704 629 149 690 769 342 538  36  51 606 834 363 413 441 158 525 266 565 304 765 473 362 319 70 310 226 179 777 480 109 505 482  22 525 575 117 761  71 723 499\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name a variable \"todo_list\" then split and transform the todo variable; the next step is to print todo_list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "todo_list=todo.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the space in the to_do list and create a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while \"\" in todo_list:\n",
    "     todo_list.remove(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Using set to reduce the repeat part and using list for data manipulating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "todo_list=list(set(todo_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The in-built function-map is to project or map one list to another and the lambda function is to implement single-line minimum function in python. Then, print the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resumes_list=[resume for resume in map(lambda x:\"resume_({})\".format(x),todo_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a regular slice from resumes which at the position 0 to position 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create resumes_to_do variable as the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resumes_to_do={}\n",
    "for path,dir_list,file_list in os.walk(os.getcwd()+\"/resumeTxt\"):\n",
    "    for file_name in file_list:\n",
    "        if file_name.split(\".\")[0] in resumes_list:\n",
    "            with open(os.getcwd()+\"/resumeTxt/\"+file_name,encoding=\"UTF-8\") as file:\n",
    "                data= file.read()\n",
    "            file.close()\n",
    "            resumes_to_do[file_name.split(\".\")[0]]=data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and see if the length is the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(todo_list))==len(resumes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the required tokenizer and get the form which needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tokenizeResume function. rid which means resume id. it will return a tupel of resume_id and a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizeResume(rid):\n",
    "    tokenized_resumes = tokenizer.tokenize(resumes_to_do[rid].lower() )\n",
    "    return (rid, tokenized_resumes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary and variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resumes_tokenized_dict = dict(tokenizeResume(rid) for rid in resumes_to_do.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize resumes\n",
    "#### Assign a empty unigram dictionary to the variable unigram_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_dic={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a loop and tokenize the key in unigram_dic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in resumes_to_do.keys():\n",
    "    unigram_dic[key]=tokenizer.tokenize(resumes_to_do[key].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out unigram_cid.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign a variable--vocabs and design it as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabs = list(chain.from_iterable(unigram_dic.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the length of the words and unique words in all resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} words in all resumes\".format(len(vocabs)))\n",
    "print(\"There are {} unique words in all resumes\".format(len(set(vocabs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert the stopword file in the jupyter notebook; open and read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords_en.txt\") as file:\n",
    "    data=file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create two variables which are stopwords_list and stopwords_en. The extend function is used to append multiple values in another sequence at the end of the list; expand the original list(stopwords_list) with a new list(stopwords_en). Using set function to avoid the repeated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words(\"english\")\n",
    "stopwords_en=data.split(\"\\n\")\n",
    "stopwords_list.extend(stopwords_en)\n",
    "stopwords_set=set(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the new list of removed stopwords which have been tokenised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_stop_removed = [w for w in vocabs if w not in stopwords_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem tokens using the Porter stemmer and then set two variables stemmer and tokens_stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "tokens_stemmed= [stemmer.stem(w) for w in tokens_stop_removed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify content dependent words and rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency_dict={}\n",
    "uniuqe_vocabs=set(vocabs)\n",
    "for vocab in uniuqe_vocabs:\n",
    "    frequency_dict[vocab]=0\n",
    "    for key in resumes_to_do.keys():\n",
    "        if vocab in resumes_to_do[key]:\n",
    "            frequency_dict[vocab]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the standard of 98% of the length of resumes_to_do in identifying the context_dependent_word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_dependent_word=[]\n",
    "for key in frequency_dict.keys():\n",
    "    if frequency_dict[key]>=0.98*len(resumes_to_do):\n",
    "        context_dependent_word.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the standard of 2% of the length of resumes_to_do in identifying the rare_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rare_words=[]\n",
    "for key in frequency_dict.keys():\n",
    "    if frequency_dict[key]<=0.02*len(resumes_to_do):\n",
    "        rare_words.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the content dependent words and the rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_dependent_word_removed = [w for w in tokens_stemmed if w not in context_dependent_word]\n",
    "rare_words_removed = [w for w in context_dependent_word_removed if w not in rare_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(rare_words_removed)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)\n",
    "bigram_finder.apply_freq_filter(20)\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-200 bigrams\n",
    "top_200_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwetokenizer = MWETokenizer(top_200_bigrams)\n",
    "colloc_resumes =  dict((rid, mwetokenizer.tokenize(resumes_tokenized)) for rid,resumes_tokenized in resumes_tokenized_dict.items())\n",
    "vocabs_colloc = list(chain.from_iterable(colloc_resumes.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The variable remove the 3 factors: 1. not in stopwords_set; 2. not in context_dependent_word; 3. not in rare_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabs_colloc_removed=[w for w in vocabs_colloc if  (w not in stopwords_set) \\\n",
    "                          and (w not in context_dependent_word) and (w not in rare_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print out the result of the length of vocabs_colloc_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocabs_colloc_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the repeated values and create a list for the variable--colloc_vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colloc_vocab = list(set(vocabs_colloc_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the file \"28464176_vocab.txt\" and design it in the write mode. Create a loop and set the default value = 0 ### and write the content into the new design i+=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_writer = open(\"28464176_vocab.txt\", \"w\")\n",
    "i=0\n",
    "for typ in colloc_vocab:\n",
    "    v_writer.write(str(i)+\":\"+typ+\"\\n\")\n",
    "    i+=1\n",
    "v_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open and write the file ; format it with the value of the zero and first position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_writer = open(\"28464176_countVec.txt\", \"w\")\n",
    "for key in colloc_resumes.keys():\n",
    "    w_writer.write(key+\".txt\")\n",
    "    sent=colloc_resumes[key]\n",
    "    fd = FreqDist(sent)\n",
    "    for word, count in fd.items():\n",
    "        if word in colloc_vocab:\n",
    "            w_writer.write(\",{0}:{1}\".format(colloc_vocab.index(word), count))\n",
    "    w_writer.write(\"\\n\")\n",
    "w_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
